import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, cross_validate
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
import warnings
import joblib
import os
import json
from datetime import datetime
warnings.filterwarnings('ignore')

class MalwareModelComparison:

    # constructure
    def __init__(self, data_path):
        """Initialize with dataset path"""
        self.data_path = data_path # path ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö dataset
        self.data = None # DataFrame ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        self.X = None # Feature matrix
        self.y = None # Target vector
        self.feature_names = None # ‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå
        self.results = {} # ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
        self.final_models = {} # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å

    def load_and_prepare_data(self): # <-- fucntion for read dataset 
        """Load and prepare the dataset"""
        print("Loading dataset...")
        self.data = pd.read_csv(self.data_path) # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV ‡πÄ‡∏õ‡πá‡∏ô DataFrame
        print(f"Dataset shape: {self.data.shape}")
        
        # if "total_permissions" not in self.data.columns: # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå total_permissions ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        #     self.data["total_permissions"] = ( # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå total_permissions
        #         self.data.get("dangerous_permissions", 0) +
        #         self.data.get("normal_permissions", 0) +
        #         self.data.get("unknown_permissions", 0)
        #     )
        #self.data = self.data[self.data['total_permissions'] > 0] # ‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ total_permissions > 0 ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡πÅ‡∏≠‡∏õ‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå dangerous normal unknown total
        #print(f"Cleaned dataset shape: {self.data.shape}")

        label_counts = self.data['label'].value_counts() # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏•‡∏≤‡∏™
        print(f"Class distribution: Safe(0): {label_counts.get(0,0)}, Malware(1): {label_counts.get(1,0)}")
        if len(self.data) > 0: # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            print(f"Malware ratio: {label_counts.get(1,0)/len(self.data):.2%}") # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á malware ‡πÅ‡∏•‡∏∞ benign ‡πÉ‡∏ô dataset
        return self.data
    
    def select_features(self):  # <-- function for select feature
        """Select relevant features for modeling"""
        exclude_cols = ['label', 'family', 'filename'] # ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå
        feature_cols = [ # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå
            col for col in self.data.columns
            if col not in exclude_cols and self.data[col].dtype in ['int64', 'float64']
        ]
        self.X = self.data[feature_cols] # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î feature matrix
        self.y = self.data['label'] # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î target vector
        self.feature_names = feature_cols # ‡πÄ‡∏Å‡πá‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå
        print(f"Selected {len(feature_cols)} features for modeling")
        return self.X, self.y # return feature matrix and target vector
    
    def train_and_evaluate_models(self, cv_folds=5): # <-- function for Test betterween xgboost and randomforest
        """Train and evaluate models using cross-validation"""
        models = { # dictionary ‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
            'Random Forest': RandomForestClassifier( # ‡πÇ‡∏°‡πÄ‡∏î‡∏• Random Forest
                n_estimators=200, # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡πÉ‡∏ô‡∏õ‡πà‡∏≤
                max_depth=10, # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∂‡∏Å‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ
                min_samples_split=5, # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á
                min_samples_leaf=2, # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡πÉ‡∏ô‡πÉ‡∏ö‡πÑ‡∏°‡πâ
                random_state=42, # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ random state ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠
                class_weight='balanced' # ‡∏õ‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™
            ),
            'XGBoost': xgb.XGBClassifier(
                n_estimators=200, # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡πÉ‡∏ô‡∏õ‡πà‡∏≤
                max_depth=6, # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∂‡∏Å‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ
                learning_rate=0.1, # ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
                subsample=0.8, # ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å
                colsample_bytree=0.8, # ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å
                random_state=42, # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ random state ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠
                scale_pos_weight=len(self.y[self.y==0])/len(self.y[self.y==1]) # ‡∏õ‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™
            )
        }

        scoring = [
            'accuracy', # ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ ‡∏Ñ‡∏∑‡∏≠ ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            'precision', # ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏ß‡∏Å ‡∏Ñ‡∏∑‡∏≠ ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏ß‡∏Å‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            'recall', # ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ß ‡∏Ñ‡∏∑‡∏≠ ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏ß‡∏Å
            'f1', # ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô F1 ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡∏Æ‡∏≤‡∏£‡πå‡∏°‡∏≠‡∏ô‡∏¥‡∏Å‡∏Ç‡∏≠‡∏á precision ‡πÅ‡∏•‡∏∞ recall
            'roc_auc' # AUC-ROC ‡∏Ñ‡∏∑‡∏≠‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ï‡πâ‡πÇ‡∏Ñ‡πâ‡∏á ROC
        ] # ‡πÄ‡∏°‡∏ï‡∏£‡∏¥‡∏Å‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42) # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î cross-validation ‡πÅ‡∏ö‡∏ö stratified
        results = {} # dictionary ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
        
        for model_name, model in models.items(): # loop ‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•
            print(f"\nTraining {model_name}...")
            cv_results = cross_validate( # ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ cross-validation 
                model, self.X, self.y,
                cv=cv, scoring=scoring, return_train_score=True
            )
            """
            5 k fold cross-validation results:
            round 1 | train = 2,3,4,5 | test = 1
            round 2 | train = 1,3,4,5 | test = 2
            round 3 | train = 1,2,4,5 | test = 3
            round 4 | train = 1,2,3,5 | test = 4
            round 5 | train = 1,2,3,4 | test = 5
            20% test 80% train 
            """
            results[model_name] = {} # ‡∏™‡∏£‡πâ‡∏≤‡∏á dictionary ‡∏¢‡πà‡∏≠‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•
            for metric in scoring:
                # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏• test
                results[model_name][f'{metric}_test'] = {
                    'mean': np.mean(cv_results[f'test_{metric}']), # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á‡πÄ‡∏°‡∏ï‡∏£‡∏¥‡∏Å
                    'std': np.std(cv_results[f'test_{metric}']), # ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏°‡∏ï‡∏£‡∏¥‡∏Å
                    'scores': cv_results[f'test_{metric}'] # ‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡πÄ‡∏°‡∏ï‡∏£‡∏¥‡∏Å‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á cross-validation
                }
                
                # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏• train
                results[model_name][f'{metric}_train'] = {
                    'mean': np.mean(cv_results[f'train_{metric}']),
                    'std': np.std(cv_results[f'train_{metric}']),
                    'scores': cv_results[f'train_{metric}']
                }
                
                gap = results[model_name][f'{metric}_train']['mean'] - results[model_name][f'{metric}_test']['mean'] # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Gap ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á train ‡∏Å‡∏±‡∏ö test 0.94 - 0.91 = 0.03
    
                # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• ‡∏û‡∏£‡πâ‡∏≠‡∏° Gap ‡πÄ‡∏õ‡πá‡∏ô %
                print(f"{metric.upper():8} | "
                    f"Train: {results[model_name][f'{metric}_train']['mean']:.3f} ¬± {results[model_name][f'{metric}_train']['std']:.3f} | " #  ‡∏ú‡∏• train mean std
                    f"Test: {results[model_name][f'{metric}_test']['mean']:.3f} ¬± {results[model_name][f'{metric}_test']['std']:.3f} | " # ‡∏ú‡∏• test mean std
                    f"Gap: {gap*100:.2f}%") # Gap ‡πÄ‡∏õ‡πá‡∏ô %
        self.results = results 
        return results
    
    def print_final_recommendation(self): # <-- function show best model 
        """Print final model recommendation"""
        if not self.results: # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            print("No results available.")
            return None, None

        models = list(self.results.keys()) # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        overall_scores = {} # dictionary ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•
        weights = {'accuracy':0.2, 'precision':0.2, 'recall':0.2, 'f1':0.3, 'roc_auc':0.1} # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏°‡∏ï‡∏£‡∏¥‡∏Å

        for model in models: # loop ‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•
            score = 0 # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°
            for metric, w in weights.items():
                score += self.results[model][f'{metric}_test']['mean'] * w # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡πÇ‡∏î‡∏¢‡∏Ñ‡∏π‡∏ì‡∏Ñ‡πà‡∏≤ mean ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏°‡∏ï‡∏£‡∏¥‡∏Å‡∏Å‡∏±‡∏ö‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
            overall_scores[model] = score # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡πÉ‡∏ô dictionary
        
        best_model = max(overall_scores, key=overall_scores.get) # ‡∏´‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
        best_score = overall_scores[best_model] # ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
        print(f"\nBest Model: {best_model} | Overall Score: {best_score:.3f}") # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°o
        
        # print("\nDetailed metrics per model:")
        # for model in models: 
        #     print(f"\n{model}:")
        #     for metric in ['accuracy','precision','recall','f1','roc_auc']:
        #         print(f"  {metric.upper():8}: {self.results[model][f'{metric}_test']['mean']:.3f} ¬± {self.results[model][f'{metric}_test']['std']:.3f}")
        #     print(f"  Weighted Overall Score: {overall_scores[model]:.3f}")
        
        return best_model, best_score
    
    def train_final_models(self): # <-- function for train final model 
        """Train both models on full dataset"""
        rf_model = RandomForestClassifier( 
            n_estimators=100,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,        
            random_state=42,
            class_weight='balanced'
        )
        xgb_model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            scale_pos_weight=len(self.y[self.y==0])/len(self.y[self.y==1])
        )
        rf_model.fit(self.X, self.y)
        xgb_model.fit(self.X, self.y)
        self.final_models = {"Random Forest": rf_model, "XGBoost": xgb_model}
        print("Final models trained.")
        return self.final_models
    
    def save_models(self, output_dir="./Model"):
        """Save models and feature names"""
        os.makedirs(output_dir, exist_ok=True) #
        for name, model in self.final_models.items():
            path = f"{output_dir}/{name.replace(' ','_')}_final.pkl"
            joblib.dump(model, path)
            print(f"Saved {name} model to {path}")
        feature_file = f"{output_dir}/feature_names.json"
        with open(feature_file, 'w') as f:
            json.dump(self.feature_names, f)
        print(f"Saved feature names to {feature_file}")

if __name__ == "__main__": # <-- Methode Main
    CSV_FILE = "./Dataset/malware_dataset.csv" 
    try:
        comparison = MalwareModelComparison(CSV_FILE)
        comparison.load_and_prepare_data()
        comparison.select_features()
        comparison.train_and_evaluate_models(cv_folds=5) # <-- Cross-validation testing
        comparison.print_final_recommendation() # <-- Final model recommendation
        comparison.train_final_models()
        comparison.save_models("./Model")
        print("\nüéâ Malware analysis completed successfully!")
    except FileNotFoundError:
        print(f"‚ùå CSV file not found: {CSV_FILE}")
    except Exception as e:
        print(f"‚ùå Error during analysis: {e}")
